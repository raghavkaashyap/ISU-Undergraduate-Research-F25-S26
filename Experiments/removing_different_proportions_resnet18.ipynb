{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "996efb8c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mF\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import copy\n",
    "import time\n",
    "\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torch import optim\n",
    "from torchvision import transforms, datasets, models\n",
    "from scipy.special import softmax\n",
    "from scipy.optimize import minimize\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f0d47a2",
   "metadata": {},
   "source": [
    "# Experiment: Does Removing More Data Increase Uncertainty? (ResNet-18)\n",
    "\n",
    "This notebook investigates whether removing different proportions of training data (5%, 10%, 15%, 20%) using MC dropout uncertainty estimation and first-order based unlearning consistently increases uncertainty.\n",
    "\n",
    "Dataset: CIFAR-10 with ResNet-18\n",
    "Uncertainty Method: MC Dropout\n",
    "Unlearning Method: First-order based\n",
    "Metrics: ECE (Expected Calibration Error) and BS (Brier Score)\n",
    "Each experiment runs 3 times with mean ± standard deviation reported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82db60d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup GPU device\n",
    "if torch.cuda.is_available():\n",
    "    cudnn.benchmark = True\n",
    "    device = \"cuda:0\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "print(f'Device: {device}')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "def set_random_seed(seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "set_random_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b0af87",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "764a7c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CIFAR-10 dataset\n",
    "data_mean = (0.4914, 0.4822, 0.4465)\n",
    "data_std = (0.2023, 0.1994, 0.2010)\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(data_mean, data_std), ])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(data_mean, data_std), ])\n",
    "\n",
    "train_data = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
    "test_set = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
    "\n",
    "cali_indices, test_indices = train_test_split(\n",
    "    range(len(test_set)),\n",
    "    test_size=0.5,\n",
    "    stratify=test_set.targets,\n",
    ")\n",
    "\n",
    "cali_data = Subset(test_set, cali_indices)\n",
    "test_data = Subset(test_set, test_indices)\n",
    "\n",
    "num_classes = 10\n",
    "batch_size = 128\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "cali_loader = DataLoader(cali_data, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(f'Training data length: {len(train_data)}, calibration data length: {len(cali_data)}, test data length: {len(test_data)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "985c2554",
   "metadata": {},
   "source": [
    "## Define ResNet-18 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a85aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ResNet-18 with MC dropout support\n",
    "class ResNet18CIFAR(nn.Module):\n",
    "    def __init__(self, num_classes=10, dropout_rate=0.5):\n",
    "        super(ResNet18CIFAR, self).__init__()\n",
    "        # Load standard ResNet-18\n",
    "        self.resnet = models.resnet18(pretrained=False)\n",
    "        # Adapt first conv layer for CIFAR (3x32x32 instead of 3x224x224)\n",
    "        self.resnet.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.resnet.maxpool = nn.Identity()  # Remove maxpool\n",
    "        # Replace final FC layer\n",
    "        self.resnet.fc = nn.Linear(512, num_classes)\n",
    "        self.dropout = nn.Dropout(p=dropout_rate)\n",
    "        \n",
    "    def forward(self, x, dropout=False):\n",
    "        x = self.resnet.conv1(x)\n",
    "        x = self.resnet.bn1(x)\n",
    "        x = self.resnet.relu(x)\n",
    "        x = self.resnet.maxpool(x)\n",
    "        x = self.resnet.layer1(x)\n",
    "        x = self.resnet.layer2(x)\n",
    "        x = self.resnet.layer3(x)\n",
    "        x = self.resnet.layer4(x)\n",
    "        x = self.resnet.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        if dropout:\n",
    "            x = self.dropout(x)\n",
    "        x = self.resnet.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c13448",
   "metadata": {},
   "source": [
    "## Training and Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d242183",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function\n",
    "def train(model, train_loader, loss_func, optimizer, epochs):\n",
    "    model.train()\n",
    "    loss_all = []\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss, n_batches, total, correct = 0.0, 0, 0, 0\n",
    "    \n",
    "        for idx, (images, labels) in enumerate(train_loader):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = loss_func(outputs, labels)\n",
    "    \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            running_loss += loss.item()\n",
    "            n_batches += 1\n",
    "    \n",
    "        loss = running_loss / n_batches\n",
    "        accuracy = 100 * correct / total\n",
    "        if epoch % 5 == 0:\n",
    "            print('Epoch %d training loss: %.3f training accuracy: %.3f%%' % (epoch, loss, accuracy))\n",
    "    \n",
    "        loss_all.append(loss)\n",
    "\n",
    "    return loss_all\n",
    "\n",
    "# Test function\n",
    "def test(model, test_loader):\n",
    "    model.eval()\n",
    "    total, correct = 0, 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for idx, (images, labels) in enumerate(test_loader):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "    return correct / total\n",
    "\n",
    "def one_hot_encode(labels, num_classes=None):\n",
    "    if num_classes is None:\n",
    "        num_classes = len(np.unique(labels))\n",
    "    return np.eye(num_classes)[labels]\n",
    "\n",
    "def get_calibration_error(probs, labels, bin_upper_bounds, num_bins):\n",
    "    if np.size(probs) == 0:\n",
    "        return 0\n",
    "\n",
    "    bin_indices = np.digitize(probs, bin_upper_bounds)\n",
    "    sums = np.bincount(bin_indices, weights=probs, minlength=num_bins)\n",
    "    sums = sums.astype(np.float64)\n",
    "    counts = np.bincount(bin_indices, minlength=num_bins)\n",
    "    counts = counts + np.finfo(sums.dtype).eps\n",
    "    confs = sums / counts\n",
    "    accs = np.bincount(bin_indices, weights=labels, minlength=num_bins) / counts\n",
    "\n",
    "    calibration_errors = accs - confs\n",
    "    weighting = counts / float(len(probs.flatten()))\n",
    "    weighted_calibration_error = calibration_errors * weighting\n",
    "\n",
    "    return np.sum(np.abs(weighted_calibration_error))\n",
    "\n",
    "def ECE(probs, labels, num_bins=10):\n",
    "    num_classes = probs.shape[1]\n",
    "    labels_matrix = one_hot_encode(labels, probs.shape[1])\n",
    "\n",
    "    bin_upper_bounds = np.histogram_bin_edges([], bins=num_bins, range=(0.0, 1.0))[1:]\n",
    "\n",
    "    labels_matrix = labels_matrix[range(len(probs)), np.argmax(probs, axis=1)]\n",
    "    probs_matrix = probs[range(len(probs)), np.argmax(probs, axis=1)]\n",
    "\n",
    "    calibration_error = get_calibration_error(probs_matrix.flatten(), labels_matrix.flatten(), bin_upper_bounds, num_bins)\n",
    "\n",
    "    return calibration_error\n",
    "\n",
    "def BS(probs, labels):\n",
    "    n_samples, n_classes = probs.shape\n",
    "    labels_matrix = one_hot_encode(labels, n_classes)\n",
    "    brier_score = np.sum((probs - labels_matrix) ** 2) / n_samples\n",
    "\n",
    "    return brier_score\n",
    "\n",
    "def get_outputs(model, data_loader):\n",
    "    model.eval()\n",
    "    all_labels, all_logits = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx, (images, labels) in enumerate(data_loader):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            all_labels.append(labels.detach().cpu().numpy())\n",
    "            all_logits.append(outputs.detach().cpu().numpy())\n",
    "\n",
    "    all_labels = np.concatenate(all_labels, axis=0)\n",
    "    all_logits = np.concatenate(all_logits, axis=0)\n",
    "\n",
    "    return all_labels, all_logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc9a30c",
   "metadata": {},
   "source": [
    "## MC Dropout Uncertainty Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b13580a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MC Dropout uncertainty estimation\n",
    "def evaluate_dropout(model, test_loader, forward_passes=10):\n",
    "    def enable_dropout(model):\n",
    "        \"\"\" Function to enable the dropout layers during test-time \"\"\"\n",
    "        for m in model.modules():\n",
    "            if m.__class__.__name__.startswith('Dropout'):\n",
    "                m.train()\n",
    "\n",
    "    def get_outputs_dropout(model, data_loader):\n",
    "        all_labels, all_logits = [], []\n",
    "        model.eval()\n",
    "        enable_dropout(model)\n",
    "        with torch.no_grad():\n",
    "            for idx, (images, labels) in enumerate(data_loader):\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images, dropout=True)\n",
    "\n",
    "                all_labels.append(labels.detach().cpu().numpy())\n",
    "                all_logits.append(outputs.detach().cpu().numpy())\n",
    "\n",
    "            all_labels = np.concatenate(all_labels, axis=0)\n",
    "            all_logits = np.concatenate(all_logits, axis=0)\n",
    "\n",
    "        return all_labels, all_logits\n",
    "\n",
    "    test_probs = []\n",
    "    for i in range(forward_passes):\n",
    "        test_labels, test_logits = get_outputs_dropout(model, test_loader)\n",
    "        test_probs.append(softmax(test_logits, axis=1))\n",
    "\n",
    "    test_probs = np.array(test_probs)\n",
    "    test_probs_mean = test_probs.mean(0)\n",
    "\n",
    "    ece = ECE(test_probs_mean, test_labels)\n",
    "    bs = BS(test_probs_mean, test_labels)\n",
    "\n",
    "    return ece, bs, test_probs_mean, test_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46dfa56c",
   "metadata": {},
   "source": [
    "## First-Order Based Unlearning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec5ff402",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First-order based unlearning\n",
    "def get_grad_diff(model, unlearn_loader):\n",
    "    loss_func = nn.CrossEntropyLoss(reduction=\"sum\")\n",
    "    model.train()\n",
    "    grads = []\n",
    "\n",
    "    for i, (images, labels) in enumerate(unlearn_loader):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        result_z = model(images)\n",
    "        loss_z = loss_func(result_z, labels)\n",
    "        loss_diff = -loss_z\n",
    "\n",
    "        differentiable_params = [p for p in model.parameters() if p.requires_grad]\n",
    "        gradients = torch.autograd.grad(loss_diff, differentiable_params)\n",
    "        grads.append(gradients)\n",
    "\n",
    "    grads = list(zip(*grads))\n",
    "    for i in range(len(grads)):\n",
    "        tmp = grads[i][0]\n",
    "        for j in range(1, len(grads[i])):\n",
    "            tmp = torch.add(tmp, grads[i][j])\n",
    "        grads[i] = tmp\n",
    "\n",
    "    return grads\n",
    "\n",
    "def first_order_unlearn(model, unlearn_loader, tau=0.00002):\n",
    "    net_unlearn = copy.deepcopy(model)\n",
    "    diff = get_grad_diff(net_unlearn, unlearn_loader)\n",
    "    d_theta = diff\n",
    "\n",
    "    net_unlearn.eval()\n",
    "    with torch.no_grad():\n",
    "        for p in net_unlearn.parameters():\n",
    "            if p.requires_grad:\n",
    "                new_p = p - tau * d_theta.pop(0)\n",
    "                p.copy_(new_p)\n",
    "\n",
    "    return net_unlearn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "633168b1",
   "metadata": {},
   "source": [
    "## Main Experiment: Remove Different Proportions of Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc1f303",
   "metadata": {},
   "outputs": [],
   "source": [
    "proportions_to_remove = [0.05, 0.10, 0.15, 0.20]  # 5%, 10%, 15%, 20%\n",
    "num_runs = 3  # Run each experiment 3 times\n",
    "epochs = 15\n",
    "lr = 0.01\n",
    "forward_passes = 10  # MC dropout forward passes\n",
    "\n",
    "results = {prop: {'ECE': [], 'BS': []} for prop in proportions_to_remove}\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"Starting Experiment: Does Removing More Data Increase Uncertainty? (ResNet-18)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for prop_to_remove in proportions_to_remove:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Testing removal proportion: {prop_to_remove*100:.0f}%\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    for run_num in range(num_runs):\n",
    "        print(f\"\\n--- Run {run_num + 1}/{num_runs} ---\")\n",
    "        set_random_seed(42 + run_num)\n",
    "\n",
    "        print(\"Training baseline model on full dataset...\")\n",
    "        model = ResNet18CIFAR(num_classes=10, dropout_rate=0.5).to(device)\n",
    "        loss_func = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=5e-4)\n",
    "        train(model, train_loader, loss_func, optimizer, epochs)\n",
    "\n",
    "        print(\"Evaluating baseline model with MC dropout...\")\n",
    "        ece_baseline, bs_baseline, _, _ = evaluate_dropout(model, test_loader, forward_passes=forward_passes)\n",
    "        print(f\"  Baseline - ECE: {ece_baseline:.4f}, BS: {bs_baseline:.4f}\")\n",
    "        \n",
    "        num_samples = len(train_data)\n",
    "        sample_size = int(num_samples * prop_to_remove)\n",
    "        \n",
    "        indices = list(range(num_samples))\n",
    "        unlearn_indices = random.sample(indices, sample_size)\n",
    "        unlearn_data = Subset(train_data, unlearn_indices)\n",
    "        \n",
    "        retain_indices = [idx for idx in range(num_samples) if idx not in unlearn_indices]\n",
    "        retain_data = Subset(train_data, retain_indices)\n",
    "        \n",
    "        unlearn_loader = DataLoader(unlearn_data, batch_size=128, shuffle=False)\n",
    "        retain_loader = DataLoader(retain_data, batch_size=128, shuffle=True)\n",
    "        \n",
    "        print(f\"Unlearning {len(unlearn_data)} samples ({prop_to_remove*100:.0f}%), retaining {len(retain_data)} samples\")\n",
    "\n",
    "        print(\"Applying first-order unlearning...\")\n",
    "        model_unlearned = first_order_unlearn(model, unlearn_loader, tau=0.00002)\n",
    "        \n",
    "        print(\"Evaluating unlearned model with MC dropout...\")\n",
    "        ece_unlearned, bs_unlearned, _, _ = evaluate_dropout(model_unlearned, test_loader, forward_passes=forward_passes)\n",
    "        print(f\"  Unlearned - ECE: {ece_unlearned:.4f}, BS: {bs_unlearned:.4f}\")\n",
    "\n",
    "        results[prop_to_remove]['ECE'].append(ece_unlearned)\n",
    "        results[prop_to_remove]['BS'].append(bs_unlearned)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Experiment completed!\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d64e7e6",
   "metadata": {},
   "source": [
    "## Results Analysis and Reporting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0880ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"SUMMARY OF RESULTS: Uncertainty Metrics After Unlearning\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nExperiment Settings:\")\n",
    "print(f\"  - Model: ResNet-18\")\n",
    "print(f\"  - Proportions removed: {[f'{p*100:.0f}%' for p in proportions_to_remove]}\")\n",
    "print(f\"  - Number of runs per proportion: {num_runs}\")\n",
    "print(f\"  - Uncertainty method: MC Dropout ({forward_passes} forward passes)\")\n",
    "print(f\"  - Unlearning method: First-order based\")\n",
    "print()\n",
    "\n",
    "stats = {}\n",
    "print(f\"{'Proportion':<12} {'ECE (mean ± std)':<24} {'BS (mean ± std)':<24}\")\n",
    "print('-' * 70)\n",
    "for prop in proportions_to_remove:\n",
    "    stats[prop] = {}\n",
    "    ece_vals = np.array(results[prop]['ECE'])\n",
    "    bs_vals = np.array(results[prop]['BS'])\n",
    "    ece_mean, ece_std = float(np.mean(ece_vals)), float(np.std(ece_vals))\n",
    "    bs_mean, bs_std = float(np.mean(bs_vals)), float(np.std(bs_vals))\n",
    "    stats[prop]['ECE'] = {'mean': ece_mean, 'std': ece_std}\n",
    "    stats[prop]['BS'] = {'mean': bs_mean, 'std': bs_std}\n",
    "    ece_label = f\"{ece_mean:.4f} ± {ece_std:.4f}\"\n",
    "    bs_label = f\"{bs_mean:.4f} ± {bs_std:.4f}\"\n",
    "    print(f\"{int(prop*100):>3}%{'':<8} {ece_label:<24} {bs_label:<24}\")\n",
    "\n",
    "print(\"\\nRaw Results (all runs):\")\n",
    "for prop in proportions_to_remove:\n",
    "    print(f\"\\nProportion: {prop*100:.0f}%\")\n",
    "    print(f\"  ECE values: {[f'{v:.4f}' for v in results[prop]['ECE']]}\")\n",
    "    print(f\"  BS values:  {[f'{v:.4f}' for v in results[prop]['BS']]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00245ef7",
   "metadata": {},
   "source": [
    "## Visualization: Line Charts with Error Bars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658172b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "proportions_pct = [p * 100 for p in proportions_to_remove]\n",
    "ece_means = [stats[prop]['ECE']['mean'] for prop in proportions_to_remove]\n",
    "ece_stds = [stats[prop]['ECE']['std'] for prop in proportions_to_remove]\n",
    "bs_means = [stats[prop]['BS']['mean'] for prop in proportions_to_remove]\n",
    "bs_stds = [stats[prop]['BS']['std'] for prop in proportions_to_remove]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "ax1 = axes[0]\n",
    "ax1.errorbar(proportions_pct, ece_means, yerr=ece_stds, fmt='o-', linewidth=2, markersize=8, \n",
    "             color='#1f77b4', ecolor='#1f77b4', capsize=5, capthick=2, label='ECE')\n",
    "ax1.set_xlabel('Proportion of Data Removed (%)', fontsize=12, fontweight='bold')\n",
    "ax1.set_ylabel('Expected Calibration Error (ECE)', fontsize=12, fontweight='bold')\n",
    "ax1.set_title('ECE vs. Data Removal Proportion (ResNet-18)\\n(MC Dropout + First-Order Unlearning)', fontsize=13, fontweight='bold')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.set_xticks(proportions_pct)\n",
    "ax1.legend(fontsize=11)\n",
    "\n",
    "for i, (x, y) in enumerate(zip(proportions_pct, ece_means)):\n",
    "    ax1.text(x, y + ece_stds[i] + 0.002, f'{y:.4f}', ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "ax2 = axes[1]\n",
    "ax2.errorbar(proportions_pct, bs_means, yerr=bs_stds, fmt='s-', linewidth=2, markersize=8,\n",
    "             color='#ff7f0e', ecolor='#ff7f0e', capsize=5, capthick=2, label='Brier Score')\n",
    "ax2.set_xlabel('Proportion of Data Removed (%)', fontsize=12, fontweight='bold')\n",
    "ax2.set_ylabel('Brier Score (BS)', fontsize=12, fontweight='bold')\n",
    "ax2.set_title('Brier Score vs. Data Removal Proportion (ResNet-18)\\n(MC Dropout + First-Order Unlearning)', fontsize=13, fontweight='bold')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.set_xticks(proportions_pct)\n",
    "ax2.legend(fontsize=11)\n",
    "\n",
    "for i, (x, y) in enumerate(zip(proportions_pct, bs_means)):\n",
    "    ax2.text(x, y + bs_stds[i] + 0.002, f'{y:.4f}', ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('uncertainty_vs_data_removal_resnet18.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nVisualization saved as 'uncertainty_vs_data_removal_resnet18.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c37f00e4",
   "metadata": {},
   "source": [
    "## Analysis and Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a722db68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"KEY FINDINGS AND ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Check if uncertainty increases with data removal\n",
    "ece_trend = ece_means[-1] - ece_means[0]\n",
    "bs_trend = bs_means[-1] - bs_means[0]\n",
    "\n",
    "print(f\"\\n1. UNCERTAINTY TRENDS:\")\n",
    "print(f\"   - ECE change (5% → 20% removal): {ece_trend:+.4f} ({(ece_trend/ece_means[0]*100):+.1f}%)\")\n",
    "print(f\"     {'↑ ECE increases (higher uncertainty)' if ece_trend > 0 else '↓ ECE decreases (lower uncertainty)'}\")\n",
    "print(f\"   - BS change (5% → 20% removal):  {bs_trend:+.4f} ({(bs_trend/bs_means[0]*100):+.1f}%)\")\n",
    "print(f\"     {'↑ BS increases (higher uncertainty)' if bs_trend > 0 else '↓ BS decreases (lower uncertainty)'}\")\n",
    "\n",
    "print(f\"\\n2. CONSISTENCY ACROSS RUNS:\")\n",
    "print(f\"   - Average ECE std dev across proportions: {np.mean(ece_stds):.4f}\")\n",
    "print(f\"   - Average BS std dev across proportions:  {np.mean(bs_stds):.4f}\")\n",
    "print(f\"   - ECE consistency: {'Good (low variance)' if np.mean(ece_stds) < 0.005 else 'Moderate' if np.mean(ece_stds) < 0.01 else 'High variance'}\")\n",
    "print(f\"   - BS consistency:  {'Good (low variance)' if np.mean(bs_stds) < 0.005 else 'Moderate' if np.mean(bs_stds) < 0.01 else 'High variance'}\")\n",
    "\n",
    "print(f\"\\n3. MONOTONICITY CHECK:\")\n",
    "# Check if trend is monotonic\n",
    "ece_monotonic = all(ece_means[i] <= ece_means[i+1] for i in range(len(ece_means)-1))\n",
    "bs_monotonic = all(bs_means[i] <= bs_means[i+1] for i in range(len(bs_means)-1))\n",
    "\n",
    "print(f\"   - ECE monotonically increases: {'Yes' if ece_monotonic else 'No (non-monotonic behavior detected)'}\")\n",
    "print(f\"   - BS monotonically increases:  {'Yes' if bs_monotonic else 'No (non-monotonic behavior detected)'}\")\n",
    "\n",
    "print(f\"\\n4. SUMMARY:\")\n",
    "if ece_trend > 0 and bs_trend > 0:\n",
    "    print(f\"   ✓ CONSISTENT: Removing more data INCREASES uncertainty (both ECE and BS)\")\n",
    "    print(f\"     This suggests that unlearning more data leads to less confident predictions.\")\n",
    "elif ece_trend < 0 and bs_trend < 0:\n",
    "    print(f\"   ✗ COUNTER-INTUITIVE: Removing more data DECREASES uncertainty\")\n",
    "    print(f\"     This suggests unexpected model behavior under unlearning.\")\n",
    "else:\n",
    "    print(f\"   ≈ MIXED RESULTS: ECE and BS show opposite trends\")\n",
    "    print(f\"     Further investigation needed.\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv (3.9.6)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
