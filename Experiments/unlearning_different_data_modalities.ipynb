{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9960d8c",
   "metadata": {},
   "source": [
    "# Experiment: Unlearning Different Data Modalities and Predictive Uncertainty\n",
    "\n",
    "We will compare how unlearning the same proportion (5%) of three different data modalities affects predictive uncertainty (ECE and Brier Score):\n",
    "\n",
    "- Random instances (5% random training samples)\n",
    "- Gaussian-noise instances (5% of samples with added Gaussian noise)\n",
    "- Modified-label instances (5% of samples with randomly flipped labels)\n",
    "\n",
    "Dataset/model: CIFAR-10 with a small CNN\n",
    "Unlearning method: First-order based (single-step gradient removal)\n",
    "Uncertainty method: Temperature scaling (evaluate ECE and Brier Score)\n",
    "\n",
    "Runs: 3 repeats per modality. The notebook will produce a results table with mean ± std for ECE and BS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb1b6759",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "import numpy as np\n",
    "import random\n",
    "import copy\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.utils.data import DataLoader, Subset, Dataset\n",
    "from torch import optim\n",
    "from torchvision import transforms, datasets\n",
    "from scipy.special import softmax\n",
    "from scipy.optimize import minimize\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    cudnn.benchmark = True\n",
    "    device = \"cuda:0\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "print('Device:', device)\n",
    "\n",
    "def set_random_seed(seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "set_random_seed(42)\n",
    "\n",
    "data_mean = (0.4914, 0.4822, 0.4465)\n",
    "data_std = (0.2023, 0.1994, 0.2010)\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(data_mean, data_std),\n",
    "])\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(data_mean, data_std),\n",
    "])\n",
    "\n",
    "train_data = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
    "test_set = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
    "\n",
    "cali_indices, test_indices = train_test_split(range(len(test_set)), test_size=0.5, stratify=test_set.targets)\n",
    "cali_data = Subset(test_set, cali_indices)\n",
    "test_data = Subset(test_set, test_indices)\n",
    "\n",
    "batch_size = 128\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "cali_loader = DataLoader(cali_data, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(f'Train: {len(train_data)}, Calib: {len(cali_data)}, Test: {len(test_data)}')\n",
    "\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, 4, stride=2, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 32, 4, stride=2, padding=1)\n",
    "        self.pool = nn.AvgPool2d(2,2)\n",
    "        self.fc1 = nn.Linear(32*4*4, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "    def forward(self, x, dropout=False):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        if dropout:\n",
    "            x = self.dropout(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "def train(model, train_loader, loss_func, optimizer, epochs=10):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        running_loss, n_batches, total, correct = 0.0, 0, 0, 0\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = loss_func(outputs, labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            n_batches += 1\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "        if (epoch+1) % max(1, epochs//5) == 0:\n",
    "            print(f'Epoch {epoch+1}/{epochs}, loss: {running_loss/n_batches:.4f}, acc: {100*correct/total:.2f}%')\n",
    "\n",
    "\n",
    "def test(model, data_loader):\n",
    "    model.eval()\n",
    "    total, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in data_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    return correct / total\n",
    "\n",
    "# ECE and BS\n",
    "\n",
    "def one_hot_encode(labels, num_classes=None):\n",
    "    if num_classes is None:\n",
    "        num_classes = len(np.unique(labels))\n",
    "    return np.eye(num_classes)[labels]\n",
    "\n",
    "def get_calibration_error(probs, labels, bin_upper_bounds, num_bins):\n",
    "    if np.size(probs) == 0:\n",
    "        return 0\n",
    "    bin_indices = np.digitize(probs, bin_upper_bounds)\n",
    "    sums = np.bincount(bin_indices, weights=probs, minlength=num_bins).astype(np.float64)\n",
    "    counts = np.bincount(bin_indices, minlength=num_bins) + np.finfo(sums.dtype).eps\n",
    "    confs = sums / counts\n",
    "    accs = np.bincount(bin_indices, weights=labels, minlength=num_bins) / counts\n",
    "    calibration_errors = accs - confs\n",
    "    weighting = counts / float(len(probs.flatten()))\n",
    "    weighted_calibration_error = calibration_errors * weighting\n",
    "    return np.sum(np.abs(weighted_calibration_error))\n",
    "\n",
    "\n",
    "def ECE(probs, labels, num_bins=10):\n",
    "    num_classes = probs.shape[1]\n",
    "    labels_matrix = one_hot_encode(labels, probs.shape[1])\n",
    "    bin_upper_bounds = np.histogram_bin_edges([], bins=num_bins, range=(0.0, 1.0))[1:]\n",
    "    labels_matrix = labels_matrix[range(len(probs)), np.argmax(probs, axis=1)]\n",
    "    probs_matrix = probs[range(len(probs)), np.argmax(probs, axis=1)]\n",
    "    calibration_error = get_calibration_error(probs_matrix.flatten(), labels_matrix.flatten(), bin_upper_bounds, num_bins)\n",
    "    return calibration_error\n",
    "\n",
    "\n",
    "def BS(probs, labels):\n",
    "    n_samples, n_classes = probs.shape\n",
    "    labels_matrix = one_hot_encode(labels, n_classes)\n",
    "    brier_score = np.sum((probs - labels_matrix) ** 2) / n_samples\n",
    "    return brier_score\n",
    "\n",
    "# Temperature scaling\n",
    "class TemperatureScaling():\n",
    "    def __init__(self, temp=1, maxiter=50, solver=\"BFGS\"):\n",
    "        self.temp = temp\n",
    "        self.maxiter = maxiter\n",
    "        self.solver = solver\n",
    "    def _loss_fun(self, x, probs, true):\n",
    "        scaled_probs = self.predict(probs, x)\n",
    "        loss = log_loss(y_true=true, y_pred=scaled_probs)\n",
    "        return loss\n",
    "    def fit(self, logits, true):\n",
    "        true = true.flatten()\n",
    "        opt = minimize(self._loss_fun, x0=1.0, args=(logits, true), options={'maxiter': self.maxiter}, method=self.solver)\n",
    "        self.temp = opt.x[0]\n",
    "        return opt\n",
    "    def predict(self, logits, temp=None):\n",
    "        if temp is None:\n",
    "            return softmax(logits / self.temp, axis=1)\n",
    "        else:\n",
    "            return softmax(logits / temp, axis=1)\n",
    "\n",
    "def get_outputs(model, data_loader, dropout=False):\n",
    "    model.eval()\n",
    "    all_labels, all_logits = [], []\n",
    "    with torch.no_grad():\n",
    "        for images, labels in data_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images, dropout=dropout)\n",
    "            all_labels.append(labels.detach().cpu().numpy())\n",
    "            all_logits.append(outputs.detach().cpu().numpy())\n",
    "    all_labels = np.concatenate(all_labels, axis=0)\n",
    "    all_logits = np.concatenate(all_logits, axis=0)\n",
    "    return all_labels, all_logits\n",
    "\n",
    "def get_grad_diff(model, unlearn_loader):\n",
    "    loss_func = nn.CrossEntropyLoss(reduction=\"sum\")\n",
    "    model.train()\n",
    "    grads = []\n",
    "    for i, (images, labels) in enumerate(unlearn_loader):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        result_z = model(images)\n",
    "        loss_z = loss_func(result_z, labels)\n",
    "        loss_diff = -loss_z\n",
    "        differentiable_params = [p for p in model.parameters() if p.requires_grad]\n",
    "        gradients = torch.autograd.grad(loss_diff, differentiable_params, retain_graph=False)\n",
    "        grads.append(gradients)\n",
    "    grads = list(zip(*grads))\n",
    "    for i in range(len(grads)):\n",
    "        tmp = grads[i][0]\n",
    "        for j in range(1, len(grads[i])):\n",
    "            tmp = torch.add(tmp, grads[i][j])\n",
    "        grads[i] = tmp\n",
    "    return grads\n",
    "\n",
    "def first_order_unlearn(model, unlearn_loader, tau=2e-5):\n",
    "    net_unlearn = copy.deepcopy(model)\n",
    "    diff = get_grad_diff(net_unlearn, unlearn_loader)\n",
    "    d_theta = diff\n",
    "    net_unlearn.eval()\n",
    "    with torch.no_grad():\n",
    "        for p in net_unlearn.parameters():\n",
    "            if p.requires_grad:\n",
    "                new_p = p - tau * d_theta.pop(0)\n",
    "                p.copy_(new_p)\n",
    "    return net_unlearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053d2b41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready — use run_modality_experiment() to run the full experiments.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class NoisyDataset(Dataset):\n",
    "    \"\"\"Wraps a dataset and applies Gaussian noise to specified indices.\"\"\"\n",
    "    def __init__(self, base_dataset, noise_indices=set(), sigma=0.1):\n",
    "        self.base = base_dataset\n",
    "        self.noise_indices = set(noise_indices)\n",
    "        self.sigma = sigma\n",
    "    def __len__(self):\n",
    "        return len(self.base)\n",
    "    def __getitem__(self, idx):\n",
    "        x, y = self.base[idx]\n",
    "        if idx in self.noise_indices:\n",
    "            noise = torch.randn_like(x) * self.sigma\n",
    "            x = x + noise\n",
    "            x = torch.clamp(x, -3.0, 3.0)\n",
    "        return x, y\n",
    "\n",
    "class LabelFlippedDataset(Dataset):\n",
    "    \"\"\"Wraps a dataset and flips labels for specified indices to random labels.\"\"\"\n",
    "    def __init__(self, base_dataset, flip_indices=set(), num_classes=10):\n",
    "        self.base = base_dataset\n",
    "        self.flip_indices = set(flip_indices)\n",
    "        self.num_classes = num_classes\n",
    "    def __len__(self):\n",
    "        return len(self.base)\n",
    "    def __getitem__(self, idx):\n",
    "        x, y = self.base[idx]\n",
    "        if idx in self.flip_indices:\n",
    "            new_y = int(random.choice([c for c in range(self.num_classes) if c != y]))\n",
    "            return x, new_y\n",
    "        return x, y\n",
    "\n",
    "def run_modality_experiment(remove_prop=0.05, modality='random', num_runs=3, epochs=8, lr=0.01, tau=2e-5):\n",
    "    \"\"\"\n",
    "    modality: 'random', 'noise', or 'label'\n",
    "    Returns: dict with lists of ECE and BS (temperature-scaled) for each run\n",
    "    \"\"\"\n",
    "    ece_list = []\n",
    "    bs_list = []\n",
    "\n",
    "    num_samples = len(train_data)\n",
    "    remove_count = int(num_samples * remove_prop)\n",
    "\n",
    "    for run in range(num_runs):\n",
    "        print(f\"Run {run+1}/{num_runs} — modality: {modality} — removing {remove_count} samples\")\n",
    "        set_random_seed(100 + run)\n",
    "\n",
    "        # Train base model on full training set\n",
    "        model = SimpleCNN().to(device)\n",
    "        loss_func = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=5e-4)\n",
    "        train(model, train_loader, loss_func, optimizer, epochs=epochs)\n",
    "\n",
    "        if modality == 'random':\n",
    "            remove_indices = set(random.sample(range(num_samples), remove_count))\n",
    "            train_mod = train_data\n",
    "        elif modality == 'noise':\n",
    "            noisy_indices = set(random.sample(range(num_samples), int(num_samples * 0.2)))\n",
    "            remove_indices = set(random.sample(list(noisy_indices), remove_count))\n",
    "            train_mod = NoisyDataset(train_data, noise_indices=noisy_indices, sigma=0.2)\n",
    "        elif modality == 'label':\n",
    "            flipped_indices = set(random.sample(range(num_samples), int(num_samples * 0.2)))\n",
    "            remove_indices = set(random.sample(list(flipped_indices), remove_count))\n",
    "            train_mod = LabelFlippedDataset(train_data, flip_indices=flipped_indices, num_classes=10)\n",
    "        else:\n",
    "            raise ValueError('Unknown modality')\n",
    "\n",
    "        unlearn_subset = Subset(train_mod, list(remove_indices))\n",
    "        retain_indices = [i for i in range(num_samples) if i not in remove_indices]\n",
    "        retain_subset = Subset(train_mod, retain_indices)\n",
    "\n",
    "        unlearn_loader = DataLoader(unlearn_subset, batch_size=128, shuffle=False)\n",
    "        retain_loader = DataLoader(retain_subset, batch_size=128, shuffle=True)\n",
    "\n",
    "        # Apply first-order unlearning\n",
    "        model_unlearned = first_order_unlearn(model, unlearn_loader, tau=tau)\n",
    "        cali_labels, cali_logits = get_outputs(model_unlearned, cali_loader, dropout=False)\n",
    "        test_labels, test_logits = get_outputs(model_unlearned, test_loader, dropout=False)\n",
    "\n",
    "        ts = TemperatureScaling()\n",
    "        try:\n",
    "            ts.fit(cali_logits, cali_labels)\n",
    "        except Exception as e:\n",
    "            print('Temperature scaling fit failed:', e)\n",
    "        test_probs_ts = ts.predict(test_logits)\n",
    "\n",
    "        ece_val = ECE(test_probs_ts, test_labels)\n",
    "        bs_val = BS(test_probs_ts, test_labels)\n",
    "\n",
    "        print(f'  Run result — ECE: {ece_val:.4f}, BS: {bs_val:.4f}')\n",
    "\n",
    "        ece_list.append(ece_val)\n",
    "        bs_list.append(bs_val)\n",
    "\n",
    "    return {'ECE': ece_list, 'BS': bs_list}\n",
    "\n",
    "smoke_params = {'remove_prop': 0.05, 'num_runs': 1, 'epochs': 2}\n",
    "print('Ready — use run_modality_experiment() to run the full experiments.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60266b5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Running modality: random\n",
      "================================================================================\n",
      "Run 1/3 — modality: random — removing 2500 samples\n",
      "Epoch 1/8, loss: 1.8239, acc: 34.23%\n",
      "Epoch 2/8, loss: 1.4093, acc: 49.34%\n",
      "Epoch 3/8, loss: 1.2813, acc: 54.39%\n",
      "Epoch 4/8, loss: 1.1773, acc: 58.18%\n",
      "Epoch 5/8, loss: 1.0792, acc: 61.73%\n",
      "Epoch 6/8, loss: 1.0024, acc: 64.59%\n",
      "Epoch 7/8, loss: 0.9386, acc: 66.92%\n",
      "Epoch 8/8, loss: 0.8808, acc: 69.04%\n",
      "  Run result — ECE: 0.0194, BS: 0.4781\n",
      "Run 2/3 — modality: random — removing 2500 samples\n",
      "Epoch 1/8, loss: 1.8795, acc: 31.87%\n",
      "Epoch 2/8, loss: 1.4490, acc: 48.21%\n",
      "Epoch 3/8, loss: 1.2933, acc: 53.74%\n",
      "Epoch 4/8, loss: 1.1838, acc: 57.84%\n",
      "Epoch 5/8, loss: 1.0964, acc: 61.10%\n",
      "Epoch 6/8, loss: 1.0248, acc: 63.76%\n",
      "Epoch 7/8, loss: 0.9562, acc: 66.17%\n",
      "Epoch 8/8, loss: 0.9083, acc: 68.02%\n",
      "  Run result — ECE: 0.0189, BS: 0.5376\n",
      "Run 3/3 — modality: random — removing 2500 samples\n",
      "Epoch 1/8, loss: 1.8323, acc: 34.11%\n",
      "Epoch 2/8, loss: 1.4443, acc: 48.35%\n",
      "Epoch 3/8, loss: 1.2873, acc: 54.10%\n",
      "Epoch 4/8, loss: 1.1954, acc: 57.48%\n",
      "Epoch 5/8, loss: 1.1161, acc: 60.29%\n",
      "Epoch 6/8, loss: 1.0512, acc: 62.57%\n",
      "Epoch 7/8, loss: 0.9987, acc: 64.70%\n",
      "Epoch 8/8, loss: 0.9399, acc: 66.94%\n",
      "  Run result — ECE: 0.0231, BS: 0.5389\n",
      "\n",
      "================================================================================\n",
      "Running modality: noise\n",
      "================================================================================\n",
      "Run 1/3 — modality: noise — removing 2500 samples\n",
      "Epoch 1/8, loss: 1.8239, acc: 34.23%\n",
      "Epoch 2/8, loss: 1.4093, acc: 49.34%\n",
      "Epoch 3/8, loss: 1.2813, acc: 54.39%\n",
      "Epoch 4/8, loss: 1.1773, acc: 58.18%\n",
      "Epoch 5/8, loss: 1.0792, acc: 61.73%\n",
      "Epoch 6/8, loss: 1.0024, acc: 64.59%\n",
      "Epoch 7/8, loss: 0.9386, acc: 66.92%\n",
      "Epoch 8/8, loss: 0.8808, acc: 69.04%\n",
      "  Run result — ECE: 0.0151, BS: 0.4780\n",
      "Run 2/3 — modality: noise — removing 2500 samples\n",
      "Epoch 1/8, loss: 1.8795, acc: 31.87%\n",
      "Epoch 2/8, loss: 1.4490, acc: 48.21%\n",
      "Epoch 3/8, loss: 1.2933, acc: 53.74%\n",
      "Epoch 4/8, loss: 1.1838, acc: 57.84%\n",
      "Epoch 5/8, loss: 1.0964, acc: 61.10%\n",
      "Epoch 6/8, loss: 1.0248, acc: 63.76%\n",
      "Epoch 7/8, loss: 0.9562, acc: 66.17%\n",
      "Epoch 8/8, loss: 0.9083, acc: 68.02%\n",
      "  Run result — ECE: 0.0236, BS: 0.5537\n",
      "Run 3/3 — modality: noise — removing 2500 samples\n",
      "Epoch 1/8, loss: 1.8323, acc: 34.11%\n",
      "Epoch 2/8, loss: 1.4443, acc: 48.35%\n",
      "Epoch 3/8, loss: 1.2873, acc: 54.10%\n",
      "Epoch 4/8, loss: 1.1954, acc: 57.48%\n",
      "Epoch 5/8, loss: 1.1161, acc: 60.29%\n",
      "Epoch 6/8, loss: 1.0512, acc: 62.57%\n",
      "Epoch 7/8, loss: 0.9987, acc: 64.70%\n",
      "Epoch 8/8, loss: 0.9399, acc: 66.94%\n",
      "  Run result — ECE: 0.0229, BS: 0.5443\n",
      "\n",
      "================================================================================\n",
      "Running modality: label\n",
      "================================================================================\n",
      "Run 1/3 — modality: label — removing 2500 samples\n",
      "Epoch 1/8, loss: 1.8239, acc: 34.23%\n",
      "Epoch 2/8, loss: 1.4093, acc: 49.34%\n",
      "Epoch 3/8, loss: 1.2813, acc: 54.39%\n",
      "Epoch 4/8, loss: 1.1773, acc: 58.18%\n",
      "Epoch 5/8, loss: 1.0792, acc: 61.73%\n",
      "Epoch 6/8, loss: 1.0024, acc: 64.59%\n",
      "Epoch 7/8, loss: 0.9386, acc: 66.92%\n",
      "Epoch 8/8, loss: 0.8808, acc: 69.04%\n",
      "  Run result — ECE: 0.0153, BS: 0.5457\n",
      "Run 2/3 — modality: label — removing 2500 samples\n",
      "Epoch 1/8, loss: 1.8795, acc: 31.87%\n",
      "Epoch 2/8, loss: 1.4490, acc: 48.21%\n",
      "Epoch 3/8, loss: 1.2933, acc: 53.74%\n",
      "Epoch 4/8, loss: 1.1838, acc: 57.84%\n",
      "Epoch 5/8, loss: 1.0964, acc: 61.10%\n",
      "Epoch 6/8, loss: 1.0248, acc: 63.76%\n",
      "Epoch 7/8, loss: 0.9562, acc: 66.17%\n",
      "Epoch 8/8, loss: 0.9083, acc: 68.02%\n",
      "  Run result — ECE: 0.0641, BS: 0.6739\n",
      "Run 3/3 — modality: label — removing 2500 samples\n",
      "Epoch 1/8, loss: 1.8323, acc: 34.11%\n",
      "Epoch 2/8, loss: 1.4443, acc: 48.35%\n",
      "Epoch 3/8, loss: 1.2873, acc: 54.10%\n",
      "Epoch 4/8, loss: 1.1954, acc: 57.48%\n",
      "Epoch 5/8, loss: 1.1161, acc: 60.29%\n",
      "Epoch 6/8, loss: 1.0512, acc: 62.57%\n",
      "Epoch 7/8, loss: 0.9987, acc: 64.70%\n",
      "Epoch 8/8, loss: 0.9399, acc: 66.94%\n",
      "  Run result — ECE: 0.0411, BS: 0.5770\n",
      "\n",
      "RESULTS SUMMARY (mean ± std) — remove 5% per modality\n",
      "Modality     ECE (mean±std)            BS (mean±std)            \n",
      "----------------------------------------------------------------------\n",
      "random        0.0205 ± 0.0019      0.5182 ± 0.0284 \n",
      "noise         0.0205 ± 0.0038      0.5253 ± 0.0337 \n",
      "label         0.0402 ± 0.0199      0.5989 ± 0.0546 \n",
      "Could not save CSV (pandas missing?): No module named 'pandas'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'random': {'ECE_vals': [0.019427095246813717,\n",
       "   0.018945580910018942,\n",
       "   0.023104862326651033],\n",
       "  'BS_vals': [0.47809621254656215, 0.5376397255667198, 0.5388568341449608],\n",
       "  'ECE_mean': 0.020492512827827897,\n",
       "  'ECE_std': 0.001857640338733675,\n",
       "  'BS_mean': 0.5181975907527475,\n",
       "  'BS_std': 0.028360309575760387},\n",
       " 'noise': {'ECE_vals': [0.015108782355983816,\n",
       "   0.023579998051875502,\n",
       "   0.02291039623717915],\n",
       "  'BS_vals': [0.47799112956282236, 0.5537104147730565, 0.5443316225776312],\n",
       "  'ECE_mean': 0.02053305888167949,\n",
       "  'ECE_std': 0.0038452718601741714,\n",
       "  'BS_mean': 0.52534438897117,\n",
       "  'BS_std': 0.033702015945706106},\n",
       " 'label': {'ECE_vals': [0.015272273083901129,\n",
       "   0.06410398255102805,\n",
       "   0.04109201401187242],\n",
       "  'BS_vals': [0.5456920014548575, 0.6739001896836126, 0.5769939098231825],\n",
       "  'ECE_mean': 0.0401560898822672,\n",
       "  'ECE_std': 0.019946443757434658,\n",
       "  'BS_mean': 0.5988620336538842,\n",
       "  'BS_std': 0.05457713814489942}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modalities = ['random', 'noise', 'label']\n",
    "all_stats = {}\n",
    "\n",
    "for mod in modalities:\n",
    "    print('\\n' + '='*80)\n",
    "    print(f'Running modality: {mod}')\n",
    "    print('='*80)\n",
    "    res = run_modality_experiment(remove_prop=0.05, modality=mod, num_runs=3, epochs=8, lr=0.01, tau=2e-5)\n",
    "\n",
    "    ece_vals = np.array(res['ECE'])\n",
    "    bs_vals = np.array(res['BS'])\n",
    "\n",
    "    ece_mean = ece_vals.mean()\n",
    "    ece_std = ece_vals.std()\n",
    "    bs_mean = bs_vals.mean()\n",
    "    bs_std = bs_vals.std()\n",
    "\n",
    "    all_stats[mod] = {\n",
    "        'ECE_vals': ece_vals.tolist(),\n",
    "        'BS_vals': bs_vals.tolist(),\n",
    "        'ECE_mean': float(ece_mean),\n",
    "        'ECE_std': float(ece_std),\n",
    "        'BS_mean': float(bs_mean),\n",
    "        'BS_std': float(bs_std)\n",
    "    }\n",
    "\n",
    "print('\\nRESULTS SUMMARY (mean ± std) — remove 5% per modality')\n",
    "print(f\"{'Modality':<12} {'ECE (mean±std)':<25} {'BS (mean±std)':<25}\")\n",
    "print('-'*70)\n",
    "for mod in modalities:\n",
    "    e_me, e_st = all_stats[mod]['ECE_mean'], all_stats[mod]['ECE_std']\n",
    "    b_me, b_st = all_stats[mod]['BS_mean'], all_stats[mod]['BS_std']\n",
    "    print(f\"{mod:<12} {e_me:7.4f} ± {e_st:<7.4f}    {b_me:7.4f} ± {b_st:<7.4f}\")\n",
    "\n",
    "all_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca586c93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RESULTS SUMMARY (remove 5% per modality)\n",
      "Modality             ECE (mean ± std)               BS (mean ± std)               \n",
      "--------------------------------------------------------------------------------\n",
      "Instances             0.0205 ± 0.0019      0.5182 ± 0.0284 \n",
      "Gaussian Noises       0.0205 ± 0.0038      0.5253 ± 0.0337 \n",
      "Modified Labels       0.0402 ± 0.0199      0.5989 ± 0.0546 \n"
     ]
    }
   ],
   "source": [
    "modalities_map = [\n",
    "    ('Instances', 'random'),\n",
    "    ('Gaussian Noises', 'noise'),\n",
    "    ('Modified Labels', 'label'),\n",
    "]\n",
    "\n",
    "if 'all_stats' not in globals():\n",
    "    print(\"No results available in `all_stats`. Run the experiment cell first to populate results.\")\n",
    "else:\n",
    "    print('\\nRESULTS SUMMARY (remove 5% per modality)')\n",
    "    print(f\"{'Modality':<20} {'ECE (mean ± std)':<30} {'BS (mean ± std)':<30}\")\n",
    "    print('-' * 80)\n",
    "    for display_name, key in modalities_map:\n",
    "        s = all_stats.get(key, all_stats.get(display_name))\n",
    "        if s is None:\n",
    "            print(f\"{display_name:<20} {'N/A':<30} {'N/A':<30}\")\n",
    "            continue\n",
    "        if 'ECE_mean' in s and 'ECE_std' in s:\n",
    "            e_me, e_st = s['ECE_mean'], s['ECE_std']\n",
    "        elif 'ECE' in s and isinstance(s['ECE'], dict):\n",
    "            e_me, e_st = s['ECE']['mean'], s['ECE']['std']\n",
    "        else:\n",
    "            e_vals = np.array(s.get('ECE_vals', s.get('ECE', [])))\n",
    "            e_me, e_st = (float(np.mean(e_vals)), float(np.std(e_vals))) if len(e_vals)>0 else (None, None)\n",
    "\n",
    "        if 'BS_mean' in s and 'BS_std' in s:\n",
    "            b_me, b_st = s['BS_mean'], s['BS_std']\n",
    "        elif 'BS' in s and isinstance(s['BS'], dict):\n",
    "            b_me, b_st = s['BS']['mean'], s['BS']['std']\n",
    "        else:\n",
    "            b_vals = np.array(s.get('BS_vals', s.get('BS', [])))\n",
    "            b_me, b_st = (float(np.mean(b_vals)), float(np.std(b_vals))) if len(b_vals)>0 else (None, None)\n",
    "\n",
    "        if None in (e_me, e_st, b_me, b_st):\n",
    "            print(f\"{display_name:<20} {'N/A':<30} {'N/A':<30}\")\n",
    "        else:\n",
    "            print(f\"{display_name:<20} {e_me:7.4f} ± {e_st:<7.4f}    {b_me:7.4f} ± {b_st:<7.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.9.6)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
