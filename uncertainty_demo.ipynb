{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNZAcX23f1kpyTIvBE2Y9G3"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["import torch\n","import numpy as np\n","from scipy.special import softmax\n","from scipy.optimize import minimize\n","from sklearn.metrics import log_loss"],"metadata":{"id":"K7J-5DsJVazc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Expected Calibration Error (ECE)"],"metadata":{"id":"PFW9XIWOTzEv"}},{"cell_type":"code","source":["def one_hot_encode(labels, num_classes=None):\n","  if num_classes is None:\n","    num_classes = len(np.unique(labels))\n","  return np.eye(num_classes)[labels]\n","\n","\n","def get_calibration_error(probs, labels, bin_upper_bounds, num_bins):\n","    if np.size(probs) == 0:\n","        return 0\n","\n","    bin_indices = np.digitize(probs, bin_upper_bounds)\n","    sums = np.bincount(bin_indices, weights=probs, minlength=num_bins)\n","    sums = sums.astype(np.float64)\n","    counts = np.bincount(bin_indices, minlength=num_bins)\n","    counts = counts + np.finfo(sums.dtype).eps\n","    confs = sums / counts\n","    accs = np.bincount(bin_indices, weights=labels, minlength=num_bins) / counts\n","\n","    calibration_errors = accs - confs\n","\n","    weighting = counts / float(len(probs.flatten()))\n","    weighted_calibration_error = calibration_errors * weighting\n","\n","    return np.sum(np.abs(weighted_calibration_error))\n","\n","def ECE(probs, labels, num_bins=10):\n","    num_classes = probs.shape[1]\n","    labels_matrix = one_hot_encode(labels, probs.shape[1])\n","\n","    bin_upper_bounds = np.histogram_bin_edges([], bins=num_bins, range=(0.0, 1.0))[1:]\n","\n","    labels_matrix = labels_matrix[range(len(probs)), np.argmax(probs, axis=1)]\n","    probs_matrix = probs[range(len(probs)), np.argmax(probs, axis=1)]\n","\n","    calibration_error = get_calibration_error(probs_matrix.flatten(), labels_matrix.flatten(), bin_upper_bounds, num_bins)\n","\n","    return calibration_error"],"metadata":{"id":"kuG2Qqu-T11j"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Brier Score (BS)"],"metadata":{"id":"lZ7EpTnLT1-n"}},{"cell_type":"code","source":["def BS(probs, labels):\n","    n_samples, n_classes = probs.shape\n","    labels_matrix = one_hot_encode(labels, n_classes)\n","    brier_score = np.sum((probs - labels_matrix) ** 2) / n_samples\n","\n","    return brier_score"],"metadata":{"id":"zoXP6q4lT5Pu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Softmax score"],"metadata":{"id":"_FspOVeBRxnb"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZQl0rVl6Rrw0","executionInfo":{"status":"ok","timestamp":1759114112850,"user_tz":300,"elapsed":5,"user":{"displayName":"Wyatt Qian","userId":"12217666207184626095"}},"outputId":"d6885775-6b0a-4cbd-c024-d7de04452aa6"},"outputs":[{"output_type":"stream","name":"stdout","text":["[[0.09003057 0.24472847 0.66524096]\n"," [0.2312239  0.62853172 0.14024438]]\n","ECE: 0.1469\n","BS: 0.7708\n"]}],"source":["def softmax_score(outputs):\n","    return softmax(outputs, axis=1)\n","\n","# input the outputs from the label\n","test_logits = np.array([[1.0, 2.0, 3.0],\n","                          [1.0, 2.0, 0.5]])\n","\n","test_probs = softmax_score(test_logits)\n","print(test_probs)\n","\n","test_labels = np.array([0, 1])\n","\n","ece = ECE(test_probs, test_labels)\n","bs = BS(test_probs, test_labels)\n","\n","print(f'ECE: {ece:.4f}')\n","print(f'BS: {bs:.4f}')"]},{"cell_type":"markdown","source":["## Deep ensemble"],"metadata":{"id":"FrFBFAvMS_N2"}},{"cell_type":"code","source":["def get_outputs(model, data_loader, device='cuda'):\n","    model.eval()\n","    all_labels, all_logits = [], []\n","\n","    with torch.no_grad():\n","        for idx, (images, labels, _) in enumerate(data_loader):\n","            images, labels = images.to(device), labels.to(device)\n","            outputs = model(images)\n","            all_labels.append(labels.detach().cpu().numpy())\n","            all_logits.append(outputs.detach().cpu().numpy())\n","\n","    all_labels = np.concatenate(all_labels, axis=0)\n","    all_logits = np.concatenate(all_logits, axis=0)\n","\n","    return all_labels, all_logits\n","\n","\n","def evaluate_ensemble(test_loader, num_ensemble=10):\n","    test_probs = [], []\n","    for i in range(num_ensemble):\n","        # load a sub model\n","        model = ...\n","\n","        test_labels, test_logits = get_outputs(model, test_loader)\n","        test_probs.append(softmax(test_logits, axis=1))\n","\n","    test_probs = np.array(test_probs)\n","    test_probs_mean, test_probs_std = test_probs.mean(0), test_probs.std(0)\n","\n","    ece_de = ECE(test_probs_mean, test_labels)\n","    bs_de = BS(test_probs_mean, test_labels)\n","\n","# train a set of sub models"],"metadata":{"id":"3EHEBAYhTD4I"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## MC dropout"],"metadata":{"id":"GgclFxK2Tf6V"}},{"cell_type":"code","source":["def evaluate_dropout(model, test_loader, forward_passes=10, device='cuda'):\n","    def enable_dropout(model):\n","        \"\"\" Function to enable the dropout layers during test-time \"\"\"\n","        for m in model.modules():\n","            if m.__class__.__name__.startswith('Dropout'):\n","                m.train()\n","\n","    def get_outputs_dropout(model, data_loader):\n","        all_labels, all_logits = [], []\n","        model.eval()\n","        enable_dropout(model)\n","        with torch.no_grad():\n","            for idx, (images, labels, _) in enumerate(data_loader):\n","                images, labels = images.to(device), labels.to(device)\n","                outputs = model(images, dropout=True)\n","\n","                all_labels.append(labels.detach().cpu().numpy())\n","                all_logits.append(outputs.detach().cpu().numpy())\n","\n","            all_labels = np.concatenate(all_labels, axis=0)\n","            all_logits = np.concatenate(all_logits, axis=0)\n","\n","        return all_labels, all_logits\n","\n","    test_probs = []\n","    for i in range(forward_passes):\n","        test_labels, test_logits = get_outputs_dropout(model, test_loader)\n","        test_probs.append(softmax(test_logits, axis=1))\n","\n","    test_probs = np.array(test_probs)\n","    test_probs_mean, test_probs_std = test_probs.mean(0), test_probs.std(0)\n","\n","    ece_mc = ECE(test_probs_mean, test_labels)\n","    ace_mc = BS(test_probs_mean, test_labels)"],"metadata":{"id":"WiBnZZafTjJy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Temperature scaling"],"metadata":{"id":"wFqjRQf6TjmL"}},{"cell_type":"code","source":["class TemperatureScaling():\n","\n","    def __init__(self, temp=1, maxiter=50, solver=\"BFGS\"):\n","        \"\"\"\n","        Initialize class\n","\n","        Params:\n","            temp (float): starting temperature, default 1\n","            maxiter (int): maximum iterations done by optimizer, however 8 iterations have been maximum.\n","        \"\"\"\n","        self.temp = temp\n","        self.maxiter = maxiter\n","        self.solver = solver\n","\n","    def _loss_fun(self, x, probs, true):\n","        # Calculates the loss using log-loss (cross-entropy loss)\n","        scaled_probs = self.predict(probs, x)\n","        loss = log_loss(y_true=true, y_pred=scaled_probs)\n","        return loss\n","\n","    # Find the temperature\n","    def fit(self, logits, true):\n","        \"\"\"\n","        Trains the model and finds optimal temperature\n","\n","        Params:\n","            logits: the output from neural network for each class (shape [samples, classes])\n","            true: one-hot-encoding of true labels.\n","\n","        Returns:\n","            the results of optimizer after minimizing is finished.\n","        \"\"\"\n","\n","        true = true.flatten()  # Flatten y_val\n","        opt = minimize(self._loss_fun, x0=1, args=(logits, true), options={'maxiter': self.maxiter}, method=self.solver)\n","        self.temp = opt.x[0]\n","\n","        return opt\n","\n","    def predict(self, logits, temp=None):\n","        \"\"\"\n","        Scales logits based on the temperature and returns calibrated probabilities\n","\n","        Params:\n","            logits: logits values of data (output from neural network) for each class (shape [samples, classes])\n","            temp: if not set use temperatures find by model or previously set.\n","\n","        Returns:\n","            calibrated probabilities (nd.array with shape [samples, classes])\n","        \"\"\"\n","\n","        if not temp:\n","            return softmax(logits / self.temp, axis=1)\n","        else:\n","            return softmax(logits / temp, axis=1)\n","\n","# input a calibration data\n","cali_logits = ...\n","cali_labels = ...\n","\n","ts = TemperatureScaling()\n","ts.fit(cali_logits, cali_labels)\n","test_probs_ts = ts.predict(test_logits)"],"metadata":{"id":"6fKGcJ8aTm1G"},"execution_count":null,"outputs":[]}]}